<!DOCTYPE html>
<html>
<head>
    <title> AnCoGen </title>
    <meta content="text/html; charset=UTF-8" http-equiv="Content-Type"/>
    <link href="./assets/katex.min.css" rel="stylesheet">
    <link href="./assets/slides.css" rel="stylesheet" type="text/css">
    <link href="./assets/grid.css" rel="stylesheet" type="text/css">
    <!-- Change equation font color defined in assests/slides.css -->
    <script type="text/javascript">
        document.documentElement.style
            .setProperty('--eq_font_color', '#004c86');
    </script>
    <script src="./assets/katex.min.js"></script>
    <script
            defer
            src="https://unpkg.com/img-comparison-slider@7/dist/index.js"
    ></script>
    <link
            href="https://unpkg.com/img-comparison-slider@7/dist/styles.css"
            rel="stylesheet"
    />

    <meta content="width=device-width, initial-scale=1" name="viewport">
    <style>
        div.scroll-container {
            background-color: #ffffff;
            overflow: auto;
            white-space: nowrap;
            padding: 5px;
        }

        div.scroll-container img {
            padding: 10px;
        }
    </style>

</head>
<body>

<textarea id="source">

class: middle, center, first-slide

<!--- 
https://katex.org/docs/supported.html#macros
--->
$$ \global\def\myza#1{{\color{blue}\mathbf{x}\_{q,~#1}^{(f)}}} $$
$$ \global\def\xaudio{{\color{red}\mathbf{x}^{(a)}}} $$
$$ \global\def\xaudioq{{\color{red}\mathbf{x}^{(a)}_q}} $$
$$ \global\def\xfactor{{\color{blue}\mathbf{x}^{(f)}}} $$
$$ \global\def\xfactorq{{\color{blue}\mathbf{x}^{(f)}_q}} $$

$$ \global\def\myzarec#1{{\color{blue}\hat{\mathbf{x}}\_{q,~#1}^{(f)}}} $$
$$ \global\def\xaudioqrec{{\color{red}\hat{\mathbf{x}}^{(a)}_q}} $$
$$ \global\def\xfactorqrec{{\color{blue}\hat{\mathbf{x}}^{(f)}_q}} $$

.center.boite-90[
# AnCoGen: <u>An</u>alysis, <u>Co</u>ntrol and <u>Gen</u>eration of Speech with a Masked Autoencoder
## **Paper ID: 5044**
]
.vspace[


]

.center[.bold[Samir Sadok$^1$], Simon Leglaive, Laurent Girin, Gaël Richard, Xavier Alameda-Pineda]

.center.bold[IEEE ICASSP 2025]

.small[
[![image](https://img.icons8.com/?size=60&id=45296&format=png&color=000000)](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10887856)
]

.grid[
.kol-1-6[
.vspace[

]
]
.kol-2-3[

.credit[$^1$Inria at Univ. Grenoble Alpes, CNRS, LJK, France]


]
.kol-1-6[
.vspace[

]
.right.width-65[![](images/logo_IETR.svg)]]

]

???
- Hello everyone, my name is Samir Sadok, and I am currently a postdoctoral researcher at Inria Grenoble.
- In this presentation, I will introduce our paper titled AnCoGen.
- This work focuses on [brief mention of the core idea of AnCoGen], and I’m excited to share our findings with you today.

---
class: middle, first-slide

## Joint work with

.grid[

.kol-1-4[
.center.width-90.circle[![](images/colab/simon.png)
<br/>
.bold[Simon Leglaive]<sup>.small[1]</sup>]
]

.kol-1-4[
.center.width-90.circle[![](images/colab/laurent_girin.jpeg)
<br/>
.bold[Laurent Girin]<sup>.small[2]</sup>]
]

.kol-1-4[
.center.width-90.circle[![](images/colab/richard-gael.jpg)
<br/>
.bold[Gaël Richard]<sup>.small[3]</sup>]
]

.kol-1-4[
.center.width-90.circle[![](images/colab/xavi-square-light.jpg)
<br>
.bold[Xavier Alameda-Pineda]<sup>.small[4]</sup>]
]

.kol-1-3[
  .small.center[<sup>1</sup> CentraleSupélec, IETR (UMR CNRS 6164), France]
]
.kol-1-3[
  .small.center[<sup>2</sup> Univ. Grenoble Alpes, CNRS, Grenoble-INP, GIPSA-lab, France]
]
.kol-1-3[
  .small.center[<sup>3</sup> LTCI, Télécom Paris, Institut polytechnique de Paris, France]
]
.kol-1-3[
]
.kol-1-3[
  .small.center[<sup>4 </sup>Inria at Univ. Grenoble Alpes, CNRS, LJK, France]
]
.kol-1-3[
]
.center[

]

]

???

- This work was carried out in collaboration with **several researchers** from different laboratories across France.
- Simon leglaive, Laurent Girin, Gaël Richard, and Xavi Alameda-Pineda.
- .bold[Their contributions were essential in shaping the outcomes of this research.]



---
class: center, middle

# Introduction to AnCoGen

???
To begin, I will introduce AnCoGen.
- In this section, I will first discuss **the motivation** behind our work—why this problem matters and what challenges we aim to address.
- Then, I will give a brief *overview* of AnCoGen and how it contributes to solving these challenges

---
## Introduction (1/2)

Today, the most effective methods for manipulating speech signals are based on .bold[deep learning].

.grid.center[
.kol-1-2[
.center.width-90[![](images/introduction/intro-icassp-1.svg)]
.caption[**Neural audio codecs:** e.g., Encodec, soundstream]

]
.kol-1-2[
.center.width-90[![](images/introduction/intro-icassp-2.svg)]
.caption[**Polyak model, LPCnet ...**]
]
]

.small-nvspace[

]

Existing neural encoder-decoder models achieve .bold[high-quality speech reconstruction] but
face some limitations (*controllability, robustness, unidirectional*).

.credit[
Défossez, A., Copet, J., Synnaeve, G., & Adi, Y. (2022). High fidelity neural audio compression. arXiv preprint arXiv:2210.13438.<br>
Zeghidour, N., Luebs, A., Omran, A., Skoglund, J., & Tagliasacchi, M. (2021). Soundstream: An end-to-end neural audio codec. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 30, 495-507.<br>
Polyak, A., Adi, Y., Copet, J., Kharitonov, E., Lakhotia, K., Hsu, W. N., ... & Dupoux, E. (2021). Speech resynthesis from discrete disentangled self-supervised representations. arXiv preprint arXiv:2104.00355.
]

<!--Significantly *lack controllability* for speech signal manipulation and robustness to *noise* and *reverberation*;-->
<!--and *unidirectional* mapping (from attributes to speech synthesis).-->

???
The most effective methods for .bold[processing and manipulating] speech signals rely on **deep learning**.
- Advances in neural audio codecs, such as Encodec and SoundStream, as well as models like Polyak and LPCNet, have significantly improved **speech reconstruction quality**.

- However, despite their success, existing neural encoder-decoder models face three major limitations.
    - First, they lack fine-grained controllability when modifying speech signals.
    - Second, they struggle with robustness in noisy and reverberant environments.
    - Finally, they are often constrained to a unidirectional mapping—from attributes to speech synthesis—without an easy way to analyze or modify the generated speech.

In this presentation, we address these challenges by introducing AnCoGen, a novel approach designed to overcome these limitations.


---
## Introduction (2/2)

We introduce AnCoGen for .bold[analyzing], .bold[controlling], and .bold[generating] speech signals.

.grid[
.center.width-70[![](images/introduction/AnCoGen-high-level-2.svg)]
.caption[Figure: Analysis, control, and generation of speech with AnCoGen.]
]

AnCoGen learns a .bold[bidirectional] mapping between a Mel-spectrogram and the speech attributes,
    hence leading to a **single/unified model** for the analysis and the resynthesis of speech.


???
To address these challenges, we introduce AnCoGen, a framework designed for analyzing, controlling, and generating speech signals.

- AnCoGen can decompose a speech signal into comprehensive set of speech attributes, representing:
&#9312; **Linguistic content**, &#9313; **prosody** (pitch and
loudness), 	&#9314; **speaker identity**, and &#9315; the **acoustic recording conditions** in terms of noise and reverberation.

As illustrated in the figure, AnCoGen unifies these three aspects within a single model.
- A key feature of our approach is that it learns a bidirectional mapping between the Mel-spectrogram and speech attributes.
- This enables not only high-quality speech synthesis from attributes but also precise analysis and modification of speech, making it a versatile tool for speech manipulation and resynthesis.

By integrating these capabilities into a single model, AnCoGen enhances both the .bold[interpretability and flexibility] of speech processing systems.

---
class: middle, center

# The Proposed Model: AnCoGen
.small[
[![image](https://img.icons8.com/?size=60&id=44900&format=png&color=000000)](https://github.com/samsad35/code-ancogen)
]

???
Now, let’s dive deeper into the architecture of AnCoGen and explore the key components that make up our model.
- I will break down its main building blocks
- and explain how they work together to achieve efficient speech analysis, control, and generation.
---
## Speech representations
AnCoGen utilizes *two speech signal representations*: <br>

.center[
.kol-1-2[
.bold[Mel-spectrogram representation (MS)] <br>
.small[(low-level representation close to the raw signal)]

.center.width-30[![](images/method/representation-mel.svg)]

.bar-left.small.left[The Mel-spectrograms are computed using the short-time Fourier transform.
- 64-ms Hann window (1024 samples at 16 kHz) and a 10-ms hop size (160 samples).
- The number of Mel-bands is set to 128.]
]
.kol-1-2[
.bold[High-level attributes representation ($A\_{1:6}$)] <br>
.small[(higher-level representation more suitable for controlling and manipulating speech)]

.center.width-30[![](images/method/representation-attr.svg)]

.bar-left-red.small.left[
$A\_1$:  the speech signal **linguistic content** (HuBERT model); <br>
$A\_2$: The **pitch** contour $f\_0$ (in Hz), using CREPE;<br>
$A\_3$: The **loudness**, here defined as the root mean square (RMS) signal;<br>
$A\_4$: The **speaker identity**;<br>
$A\_5$: The **signal-to-noise** ratio (SNR; in dB);<br>
$A\_6$: The **clarity index** at 50 ms (C50; in dB);

]
]
]

???
In AnCoGen, we rely on two **complementary** representations of speech signals to enable both analysis and control.

- The first is the Mel-spectrogram representation, a low-level feature that closely follows the raw signal.
We compute it using the short-time Fourier transform. The number of Mel bands is set to 128.

- The second is a higher-level attributes representation, denoted as A₁:₆, which is more suitable for controlling and manipulating speech.
This set of attributes includes:A₁, the linguistic content extracted using the HuBERT model; A₂, the pitch contour estimated with CREPE; A₃, the loudness measured as the root mean square (RMS) of the signal; A₄, the speaker identity; A₅, the signal-to-noise ratio (SNR in dB); and A₆, the clarity index at 50 ms (C50 in dB).

---
## Token representations

.center[
.kol-1-2[
.bold[Mel-spectrogram representation (MS)] <br>

.center.width-70[![](images/method/token-mel.svg)]

<hr>
.bold[High-level attributes representation ($A\_{1:6}$)] <br>

.center.width-70[![](images/method/token-attr.svg)]
]
]

.center[
.kol-1-2[
 <br>
.bar-left[
The Mel-spectrogram discrete tokens are obtained from the discrete latent
representation of a pre-trained and frozen *vector-quantized variational autoencoder (VQ-VAE)*.
]
<br>
<br>
.vspace[

]


.bar-left-red[
The speech attribute discrete tokens result from the *quantization* of the six speech attributes $A\_{1:6}$
]

]
]

???
- Attributes A2 (f0), A3 (loudness), A5 (SNR), and A6 (C50) are 1D
real-valued sequences that are normalized, resampled (if needed to
have the same length), and rounded to obtain sequences of discrete
integer values. Each token ai,t for i ∈ {2, 3, 5, 6} is then obtained
by grouping Di contiguous discrete values from the corresponding
sequence.
- Attributes A1 (HuBERT representation) and A4 (speaker
identity embedding vector) are quantized using the k-means algorithm,
as in [23]. The number of clusters is set to K1 = 100 for A1 and to the
number of speakers in the training dataset for K4.
---
## Masking strategy
Training AnCoGen is achieved by **randomly masking** the input sequences of mel-spectrogram and speech attributes tokens.


.kol-2-3[
.center.width-70[![](images/method/masking.svg)]
]
.kol-1-3[
<br>
<br>

.boite-90[
.bold[“Coupled” masking strategy $^*$]

The masking ratio for these two speech representations is drawn randomly according to a **uniform distribution** on the 1-simplex.
]
]

.credit[$^*$After this masking strategy, “all or nothing” masking
strategy is applied, in which one of the two speech representations
(MS or SAs) is entirely masked out and the other remains entirely
visible (i.e., p = 0 or 1)
]

???
.small[
- if $p \times 100 \%$ of the mel-spectrogram tokens are masked,
- then $(1 − p) \times 100 \%$ of the speech attributes tokens are masked,

where $p$ is distributed uniformly between 0 and 1.

- Training AnCoGen involves a masking strategy. The input sequences of both Mel-spectrogram and speech attribute tokens are randomly masked during training.

- We use a ‘coupled’ masking strategy, where the masking ratio for the two speech representations is randomly selected from a uniform distribution over the 1-simplex.
- This means that both Mel-spectrogram and speech attribute tokens are masked to varying degrees in each training step.

- After applying the coupled masking, we use an ‘all or nothing’ strategy, where one of the two speech representations—either the Mel-spectrogram or the speech attributes—is fully masked out, while the other remains completely visible. This ensures that the model learns to rely on both representations in different combinations, further enhancing its flexibility and robustness
]
---
## Encoder-decoder architecture
We use a **masked autoencoder (MAE)** .blue.small[(He et al., 2022)] model to learn a bidirectional mapping between a Mel-spectrogram and the speech
attributes.

.grid[
.kol-3-4[
.center.width-70[![](images/method/encoder-decoder.svg)]
.caption[Encoder-decoder architecture]
]
.kol-1-4[
.center.width-60[![](images/method/legend.svg)]
.caption[Legend]
]
]
.grid[
.boite-90[
- The encoder and decoder of AnCoGen each contain **12 multi-head self-attention**.
- The model (encoder, decoder, codebooks and mask token) is trained by computing the **cross-entropy loss** between the predicted and ground-truth discrete tokens.
]
]

.credit[
He, K. et al., (2022). Masked autoencoders are scalable vision learners. IEEE/CVF conference on computer vision and pattern recognition.<br>
Alexey Dosovitskiy, et al., 2020. An image is worth 16x16 words: Transformers for image recognition at scale. International Conference on Learning Representations (ICLR).
]

???
.boite-90[
- The encoder and decoder of AnCoGen each contain **12 multi-head self-attention** (MHSA) blocks from the Vision Transformer (ViT) .blue.small[(He et al., 2017)].
- The model (encoder, decoder, codebooks and mask token) is trained by computing the **cross-entropy loss** between the predicted and ground-truth discrete tokens.
]

.small[
- For learning a bidirectional mapping between the Mel-spectrogram and speech attributes, we adopt a masked autoencoder (MAE) model, inspired by He et al. (2022), which allows us to effectively capture the relationships between these two representations.

The architecture of AnCoGen follows a standard encoder-decoder setup, where both the encoder and decoder consist of 12 layers of multi-head self-attention. This enables the model to capture complex dependencies within the speech signals and attributes.

During training, the encoder-decoder, codebooks, and mask tokens are optimized by minimizing the cross-entropy loss between the predicted and ground-truth discrete tokens, ensuring that the model learns accurate and meaningful representations of both the Mel-spectrogram and speech attributes.

]

---
class: middle, center, blue-slide

.caption[Overall architecture of AnCoGen, to read from left (input) to right (output).]

.center.width-100[![](images/method/overall.svg)]

.center.width-80[![](images/method/legend_2.svg)]

???
- This slide presents the overall architecture of AnCoGen, which you can follow from left to right, starting from the input and moving towards the output.

- On the left side, we begin with both the Mel-spectrogram and the high-level speech attributes. After tokenization and masking, These representations are then passed through the encoder-decoder architecture, where the model learns a bidirectional mapping between the Mel-spectrogram and speech attributes.
    

---
class: middle, blue-slide
## AnCoGen at inference

The masking depends on the task: .boxed[**speech analysis**]

.center.width-80[![](images/analyse.svg)]

For .bold[speech analysis], the speech attributes are not available,
and they are thus totally masked at the input and estimated by the *Melspectrogram-to-speech attributes* mapping.

---
class: middle, blue-slide
## AnCoGen at inference

The masking depends on the task: .boxed[**speech generation**]

.center.width-80[![](images/generation.svg)]

For .bold[speech generation], the speech mel-spectrogram are not available,
and they are thus totally masked at the input and estimated by the *speech attributes-to-Melspectrogram* mapping.

---
class: middle, blue-slide
## AnCoGen at inference

The masking depends on the task: .boxed[**speech control**]

.center.width-80[![](images/analyse-control-generation.svg)]

For .bold[speech control], the pitch attributes are *controlled* between the analysis and generation stages in order to perform the pitch shifting for example.

---
class: middle

.center[
# Experiments
.small[
[![image](https://img.icons8.com/?size=60&id=43738&format=png&color=000000)](https://samsad35.github.io/site-ancogen/)
]

]

---
class: middle
## Experimental setup
.bar-left-gray[
*1) For the .bold[analysis-resynthesis] & .bold[robust $f\_0$ estimation]:*
.small[
AnCoGen is trained on synthetic **noisy** and **reverberant** speech data.

- LibriSpeech-100 dataset .blue.small[(panayotov et al., 2015)] + the noise signals from the DEMAND dataset .small.blue[(thiemann et al., 2013)];
- The signal-to-noise ratio (SNR) was varied between 0 and 30 dB;
- and synthetic reverberation was added.
]
]

.small-vspace[

]

.bar-left-gray[
*2) For the .bold[analysis-control-resynthesis (speech enhancement)]:*
.small[
- .bold[Matched conditions]: AnCoGen is trained and evaluated on the single-speaker version of the **LibriMix dataset**;
- .bold[Mismatched noise conditions]: we also compute the performance on the LibriSpeech + DEMAND dataset.
]
]

.credit[
Panayotov, et al., (2015, April). Librispeech: an asr corpus based on public domain audio books. In 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE. <br>
Thiemann, J., et al.,. (2013, June). The diverse environments multi-channel acoustic noise database (demand): A database of multichannel environmental noise recordings. Meetings on Acoustics. AIP Publishing.
]
---
## Analysis (1/2)

We present *analysis results*, which correspond to the estimation of the speech attributes from a Mel-spectrogram.

.grid[
.center.kol-2-3[
.center[
<audio controls style="height: 20px;" src="images/analyzer/example 1/audio_original.wav"></audio>
.nvspace[
]
<div class="scroll-container">
  <img src="images/analyzer/example 1/pitch.png" alt="Cinque Terre" width="600" height="400">
  <img src="images/analyzer/example 1/loudness.png" alt="Forest" width="600" height="400">
<!--  <img src="images/analyzer/example 1/identity.png" width="600" height="400">-->
  <img src="images/analyzer/example 1/content.png" width="600" height="400">
  <img src="images/analyzer/example 1/c50.png" width="600" height="400">
  <img src="images/analyzer/example 1/snr.png" width="600" height="400">
</div>
]
]
.kol-1-3[
.boite-figure[
.center.width-80[![](images/analyzer/analysis.svg)]
]
.footnote.center.small[.blue[Blue line] represents the ground-truth attribute; and the .red[red line] represents the prediction of AnCoGen.]

]
]


---
## Analysis (2/2)

For the robust $f\_0$ estimation experiment, we mixed the clean speech signals of the PTDB-TUG database with cafeteria noise from the DEMAND dataset.

.grid[
.kol-2-3[
.center.width-90[![](images/analyzer/pitch_estimation.svg)]
.caption[
 $f\_0$ estimation results.
]
]
.kol-1-3[
.boite-figure[
.center.width-80[![](images/analyzer/analysis.svg)]
]
]
]

.alert[
 AnCoGen shows very accurate $f\_0$ estimation performance, matching that
of CREPE on clean data, and maintaining very good accuracy in noisy and reverberant condition.
]

---
## Analysis-resynthesis (1/2)
Using AnCoGen to map a Mel-spectrogram to the corresponding speech attributes *(analysis stage)* and then back to the Mel-spectrogram *(generation stage)*.

.grid[
.center.kol-2-3[

.boite.grid[
.center.kol-1-2[
.center.width-100[![](images/generate/example-1/audio_original.wav.svg)]
<audio controls style="height: 20px;" src="images/generate/example-1/audio_original.wav"></audio>
.small.boxed[Original]
]
.center.kol-1-2[
.center.width-95[![](images/generate/example-1/audio_predicted.wav.svg)]
<audio controls style="height: 20px;" src="images/generate/example-1/audio_predicted.wav"></audio>
.small.boxed[Reconstruction with AnCoGen]
]
]

]
.kol-1-3[
.boite-figure[
.center.width-90[![](images/generate/analysis-generation.svg)]
]
]
]



---
## Analysis-resynthesis (2/2)
Using AnCoGen to map a Mel-spectrogram to the corresponding speech attributes *(analysis stage)* and then back to the Mel-spectrogram *(generation stage)*.

.grid[
.center.kol-2-3[

.boite.grid[
.center.kol-1-2[
.center.width-100[![](images/generate/example-2/audio_original.wav.svg)]
<audio controls style="height: 20px;" src="images/generate/example-2/audio_original.wav"></audio>
.small.boxed[Original]
]
.center.kol-1-2[
.center.width-95[![](images/generate/example-2/audio_predicted.wav.svg)]
<audio controls style="height: 20px;" src="images/generate/example-2/audio_predicted.wav"></audio>
.small.boxed[Reconstruction with AnCoGen]
]
]

]
.kol-1-3[
.boite-figure[
.center.width-90[![](images/generate/analysis-generation.svg)]
]
]
]

---
## Speech Enhancement

This section presents analysis, transformation, and synthesis results, where the speech attributes are controlled between the analysis and generation stages in order to perform speech denoising (**by increasing the SNR attribute**).
.grid[
.center.kol-2-3[

.left[
- AnCoGen effectively balances **noise reduction**, **speech quality**, and **intelligibility**.

- However, it ranks last in *speaker identity preservation* (as measured by COS).
]
]
.kol-1-3[
.boite-figure[
.center.width-90[![](images/analyzer/speech_enhancement.svg)]
]
]
]

.nvspace[

]

.center.width-90[![](images/controler/speech_enhanecement.png)]
.caption[Speech denoising results (best score in each column is in bold, second best score is underlined).]




---
class: middle

## Speech Enhancement

**From LibrisSpeech dataset + Demand**

.center.boite.grid[
.kol-1-3[
.small.boxed[Ground-truth clean speech]
<audio controls style="height: 20px;" src="images/controler/speech-enhancement/Demand/example-1/ref.wav"></audio>
.small-vspace[

]
<audio controls style="height: 20px;" src="images/controler/speech-enhancement/Demand/example-2/ref.wav"></audio>
.small-vspace[

]
<audio controls style="height: 20px;" src="images/controler/speech-enhancement/Demand/example-3/ref.wav"></audio>
]
.kol-1-3[
.small.boxed[Noisy speech]
<audio controls style="height: 20px;" src="images/controler/speech-enhancement/Demand/example-1/original.wav"></audio>
.small-vspace[

]
<audio controls style="height: 20px;" src="images/controler/speech-enhancement/Demand/example-2/original.wav"></audio>
.small-vspace[

]
<audio controls style="height: 20px;" src="images/controler/speech-enhancement/Demand/example-3/original.wav"></audio>
]
.kol-1-3[
.small.boxed[Estimated clean speech]
<audio controls style="height: 20px;" src="images/controler/speech-enhancement/Demand/example-1/rec.wav"></audio>
.small-vspace[

]
<audio controls style="height: 20px;" src="images/controler/speech-enhancement/Demand/example-2/rec.wav"></audio>
.small-vspace[

]
<audio controls style="height: 20px;" src="images/controler/speech-enhancement/Demand/example-3/rec.wav"></audio>
]
]


---
class: middle, center

# Conclusion

---
class: middle

## Resume

We introduce, .bold[AnCoGen], a self-supervised method based on masked modeling. This approach embodies $\texttt{analysis-generation-control}$ within a simple unified model.

- This model makes it possible to $\texttt{analyze}$ the speech signal, i.e., extracting pitch, loudness, identity, noise level, and content;
- $\texttt{Generate}$ audio speech from the factors provided in the analysis step;
- and provides $\texttt{control}$ by manipulating the extracted factors.

.center[

.small[

[![Project page](https://img.icons8.com/?size=60&id=43738&format=png&color=000000)](https://samsad35.github.io/site-ancogen/)
[![image](https://img.icons8.com/?size=60&id=44900&format=png&color=000000)](https://github.com/samsad35/code-ancogen)

]

]

---
class: middle, center, gray-slide

# Supplementary materials


---
class: middle, gray-slide
## Pitch manipulation

.center.width-100[![](images/controler/pitch-controller/pitch_manipulation.png)]


---
class: middle, gray-slide

## De-reverberation
**From LibrisSpeech dataset + Demand (De-reverberation)**

.center.boite.grid[
.kol-1-3[
.boxed[Original]
<audio controls style="height: 20px;" src="images/controler/speech-enhancement/Reverb/example-1/ref.wav"></audio>
.small-vspace[

]
<audio controls style="height: 20px;" src="images/controler/speech-enhancement/Reverb/example-2/ref.wav"></audio>
.small-vspace[

]
<audio controls style="height: 20px;" src="images/controler/speech-enhancement/Reverb/example-3/ref.wav"></audio>
]
.kol-1-3[
.boxed[W/ reverb]
<audio controls style="height: 20px;" src="images/controler/speech-enhancement/Reverb/example-1/original.wav"></audio>
.small-vspace[

]
<audio controls style="height: 20px;" src="images/controler/speech-enhancement/Reverb/example-2/original.wav"></audio>
.small-vspace[

]
<audio controls style="height: 20px;" src="images/controler/speech-enhancement/Reverb/example-3/original.wav"></audio>
]
.kol-1-3[
.boxed[Enhanced with AnCoGen]
<audio controls style="height: 20px;" src="images/controler/speech-enhancement/Reverb/example-1/rec.wav"></audio>
.small-vspace[

]
<audio controls style="height: 20px;" src="images/controler/speech-enhancement/Reverb/example-2/rec.wav"></audio>
.small-vspace[

]
<audio controls style="height: 20px;" src="images/controler/speech-enhancement/Reverb/example-3/rec.wav"></audio>
]
]


---
class: middle, gray-slide
## Voice conversion

.center.width-100[![](images/controler/controler-conversion.svg)]

---
class: middle, gray-slide
## Voice conversion

.center.boite.grid[
.kol-1-3[
.small.boxed[Target identity]
<audio controls style="height: 20px;" src="images/controler/voice-conversion/example_1/2-target_predicted.wav"></audio>
.small-vspace[

]
<audio controls style="height: 20px;" src="images/controler/voice-conversion/example_2/2-target_predicted.wav"></audio>
.small-vspace[

]
<audio controls style="height: 20px;" src="images/controler/voice-conversion/example_3/2-target_predicted.wav"></audio>
.small-vspace[

]
<audio controls style="height: 20px;" src="images/controler/voice-conversion/example_4/2-target_predicted.wav"></audio>
.small-vspace[

]
<audio controls style="height: 20px;" src="images/controler/voice-conversion/example_5/2-target_predicted.wav"></audio>
]
.kol-1-3[
.small.boxed[Source signal]
<audio controls style="height: 20px;" src="images/controler/voice-conversion/example_1/1-source_predicted.wav"></audio>
.small-vspace[

]
<audio controls style="height: 20px;" src="images/controler/voice-conversion/example_2/1-source_predicted.wav"></audio>
.small-vspace[

]
<audio controls style="height: 20px;" src="images/controler/voice-conversion/example_3/1-source_predicted.wav"></audio>
.small-vspace[

]
<audio controls style="height: 20px;" src="images/controler/voice-conversion/example_4/1-source_predicted.wav"></audio>
.small-vspace[

]
<audio controls style="height: 20px;" src="images/controler/voice-conversion/example_5/1-source_predicted.wav"></audio>
]
.kol-1-3[
.small.boxed[Voice conversion with AnCoGen]
<audio controls style="height: 20px;" src="images/controler/voice-conversion/example_1/3-source_to_target_predicted.wav"></audio>
.small-vspace[

]
<audio controls style="height: 20px;" src="images/controler/voice-conversion/example_2/3-source_to_target_predicted.wav"></audio>
.small-vspace[

]
<audio controls style="height: 20px;" src="images/controler/voice-conversion/example_3/3-source_to_target_predicted.wav"></audio>
.small-vspace[

]
<audio controls style="height: 20px;" src="images/controler/voice-conversion/example_4/3-source_to_target_predicted.wav"></audio>
.small-vspace[

]
<audio controls style="height: 20px;" src="images/controler/voice-conversion/example_5/3-source_to_target_predicted.wav"></audio>
]
]

</textarea>


<script src="./assets/remark-latest.min.js"></script>
<script src="./assets/auto-render.min.js"></script>
<script src="./assets/katex.min.js"></script>
<script type="text/javascript">
    function getParameterByName(name, url) {
        if (!url) url = window.location.href;
        name = name.replace(/[\[\]]/g, "\\$&");
        var regex = new RegExp("[?&]" + name + "(=([^&#]*)|&|#|$)"),
            results = regex.exec(url);
        if (!results) return null;
        if (!results[2]) return '';
        return decodeURIComponent(results[2].replace(/\+/g, " "));
    }

    var options = {
        sourceUrl: getParameterByName("p"),
        highlightLanguage: "python",
        // highlightStyle: "tomorrow",
        // highlightStyle: "default",
        highlightStyle: "github",
        // highlightStyle: "googlecode",
        // highlightStyle: "zenburn",
        highlightSpans: true,
        highlightLines: true,
        ratio: "16:9"
    };

    var renderMath = function () {
        renderMathInElement(document.body, {
            delimiters: [ // mind the order of delimiters(!?)
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false},
                {left: "\\[", right: "\\]", display: true},
                {left: "\\(", right: "\\)", display: false},
            ]
        });
    }
    var slideshow = remark.create(options, renderMath);

</script>
</body>
</html>